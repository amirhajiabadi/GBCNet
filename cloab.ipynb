{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRufB7wzskD2",
        "outputId": "e2f604bc-712a-4f80-c4ef-80685fe9d2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "\n",
        "# Directory where the images and density maps are stored\n",
        "data_dir = \"/content/drive/MyDrive/grape\"\n",
        "\n",
        "# Get a list of filenames for the input images and density maps\n",
        "image_filenames = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "density_map_filenames = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.h5')]\n",
        "\n",
        "# Load the images and their corresponding density maps\n",
        "images = []\n",
        "density_maps = []\n",
        "for image_filename, density_map_filename in zip(image_filenames, density_map_filenames):\n",
        "    # Load the image and resize to (224,224)\n",
        "    image = Image.open(image_filename)\n",
        "    image = image.convert('RGB')\n",
        "    image = image.resize((224,224))\n",
        "    \n",
        "    # Convert the image to a numpy array\n",
        "    x = img_to_array(image)\n",
        "    # Preprocess the input by subtracting the mean RGB pixel intensity of the ImageNet dataset\n",
        "    x = tf.keras.applications.vgg16.preprocess_input(x)\n",
        "    images.append(x)\n",
        "\n",
        "    # Load the density map and resize to (7,7,1)\n",
        "    with h5py.File(density_map_filename, 'r') as f:\n",
        "        density_map = np.array(f['density'])\n",
        "        density_map = Image.fromarray(density_map)\n",
        "        # Resize density map to (7,7)\n",
        "        density_map = np.array(density_map)\n",
        "        density_map = cv2.resize(density_map, (7,7))\n",
        "        # Reshape density map to (7,7,1)\n",
        "        density_map = np.reshape(density_map, (7,7,1))\n",
        "        #density_map = np.array(density_map)\n",
        "        density_maps.append(density_map)\n"
      ],
      "metadata": {
        "id": "1pNlxQiLslW-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=np.array(images)\n",
        "target=np.array(density_maps)"
      ],
      "metadata": {
        "id": "d3fhqS1osv1y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_float = features.astype('float32') # convert to float32\n",
        "target_float = target.astype('float32') # convert to float32"
      ],
      "metadata": {
        "id": "eI413pqSs0fs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_float,target_float, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the sizes of the resulting arrays\n",
        "# print(\"X_train shape:\", X_train.shape)\n",
        "# print(\"y_train shape:\", y_train.shape)\n",
        "# print(\"X_test shape:\", X_test.shape)\n",
        "# print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "WWPcF2X1s17t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "A81034--s3SM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Activation\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Reshape X_train and y_train to match the input and output shape of the model\n",
        "X_train = tf.reshape(X_train, (-1, X_train.shape[1], X_train.shape[2], 3))\n",
        "y_train = tf.reshape(y_train, (-1, y_train.shape[1], y_train.shape[2], 1))\n",
        "\n",
        "# Define the GBCNet model\n",
        "vgg = VGG16(weights='imagenet', include_top=False, input_shape=X_train[0].shape)\n",
        "\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "input_layer = vgg.input\n",
        "block5_pool = vgg.get_layer('block5_pool').output\n",
        "\n",
        "conv1 = Conv2D(512, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(block5_pool)\n",
        "conv2 = Conv2D(512, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(conv1)\n",
        "conv3 = Conv2D(512, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(conv2)\n",
        "conv4 = Conv2D(256, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(conv3)\n",
        "conv5 = Conv2D(128, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(conv4)\n",
        "conv6 = Conv2D(64, (3,3), dilation_rate=(2,2), padding='same', activation='relu')(conv5)\n",
        "conv7 = Conv2D(1, (3,3), dilation_rate=(2,2), padding='same', activation='linear')(conv6)\n",
        "\n",
        "# Remove the UpSampling2D layer and the second Activation layer, and add a Reshape layer to reshape the output to (7,7,1)\n",
        "output = Reshape((7,7,1))(conv7)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='MAE', optimizer=Adam(learning_rate=1e-4))\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qnFtFEus4eV",
        "outputId": "6bb83da0-edfc-4e12-8a97-eb0d27e8005f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.4606\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.6557\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.3442\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.1337\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.1415\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.1068\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0720\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0658\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0585\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0498\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0397\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0295\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0266\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.0255\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.0218\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0178\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0156\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0144\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.0126\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.0114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee1da881f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.reshape(y_pred, (4, 7, 7))\n",
        "\n",
        "y_test = y_test.reshape((y_test.shape[0], -1))\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred.reshape((y_pred.shape[0], -1)))\n",
        "mse = mean_squared_error(y_test, y_pred.reshape((y_pred.shape[0], -1)))\n",
        "\n",
        "print(\"Mean Absolute Error: \", mae)\n",
        "print(\"Mean Squared Error: \", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1lzmSrIwPN7",
        "outputId": "7817e1f4-8961-4893-bfa7-22812bdbe889"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Mean Absolute Error:  0.030370787\n",
            "Mean Squared Error:  0.001675114\n"
          ]
        }
      ]
    }
  ]
}